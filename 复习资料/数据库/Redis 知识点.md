## 1、Redis 数据类型及底层数据结构

### （1）数据类型和数据结构
Redis 支持以下 5 种基本数据类型 和 3 种扩展数据类型（从 Redis 5.0 开始）：

**基本数据类型：**
- String（字符串）
- List（列表）
- Set（集合）
- Hash（哈希）
- Sorted Set（有序集合，ZSet）

扩展数据类型（Redis 5.0+）：
- Stream（流）
- Bitmap（位图） —— 实际是 String 的位操作
- HyperLogLog（基数统计） —— 也是基于 String 的特殊编码

**Redis 底层数据结构（内部编码）**

Redis 每种数据类型在底层可能使用不同的数据结构来实现，以在不同场景下优化性能和内存。以下是主要的底层数据结构：

- **SDS（Simple Dynamic String）**：Redis 自定义的动态字符串，用于实现 String 类型，支持 O(1) 获取长度、二进制安全、自动扩容等
  
- **ziplist（压缩列表）**：紧凑的连续内存结构，用于小规模 List、Hash、ZSet，节省内存

- **linkedlist（双向链表）**：传统链表，用于 List（旧版本），现多被 quicklist 替代

- **quicklist**：ziplist + linkedlist 的混合结构，Redis 3.2+ 中 List 的底层实现

- **intset（整数集合）**：存储整数的有序集合，当 Set 元素全是整数且数量少时使用

- **hashtable（哈希表）**：标准的哈希表，用于 Hash、Set、ZSet 等类型的底层实现

- **skiplist（跳跃表）**：用于实现 Sorted Set，支持 O(log N) 的插入、删除、范围查询

- **listpack**：Redis 7.0 引入，替代 ziplist，更紧凑、安全

**Redis 会根据数据大小、元素数量等条件自动选择合适的底层编码（可通过 OBJECT ENCODING key 查看）。**

### （2）各数据类型与底层实现的对应关系

如下图：

![alt text](/复习资料/数据库/img/image-7.png)

**Redis 数据类型及其数据结构实现，参考**：https://pdai.tech/md/db/nosql-redis/db-redis-overview.html


### （3）各数据类型的使用场景

#### （1）String

底层：SDS（或 int）
**使用场景：**
- 缓存简单对象（如用户信息 JSON 字符串）
- 计数器（INCR/DECR）
- 分布式锁（SET key value NX EX）
- Session 存储

#### （2）List

底层：quicklist

**使用场景：**
- 消息队列（LPUSH + BRPOP）
- 最新 N 条数据（如朋友圈动态、日志）
- 栈或队列结构

#### （3）Hash

底层：ziplist / hashtable

**使用场景：**
- 存储对象（如用户资料：name, age, email）
- 避免序列化整个对象，可单独更新字段（HSET user:1001 name "Alice"），节省内存（相比多个 String）

#### （4）Set

底层：intset / hashtable

**使用场景：**
- 去重（如用户标签、好友列表）
- 集合运算（交集、并集、差集）—— 如共同关注、推荐系统
- 随机抽取（SRANDMEMBER）

#### （5）Sorted Set（ZSet）

底层：ziplist / skiplist + hashtable

**使用场景：**
- 排行榜（按分数排序，如游戏积分、热销商品）
- 延时任务（score 为时间戳，ZRANGEBYSCORE 获取到期任务）
- 带权重的队列

#### （6）Stream

底层：listpack + radix tree

**使用场景：**
- 消息队列（支持消费者组、ACK 机制），替代 Kafka 轻量级场景
- 事件日志存储

#### （7） Bitmap

底层：String（位操作）

**使用场景：**
- 用户签到（365 天只需 365 位 ≈ 46 字节）
- 活跃用户统计（BITCOUNT）
- 布隆过滤器基础

#### （8）HyperLogLog

底层：String（HLL 编码）

**使用场景：**
- UV 统计（独立访客数）
- 基数估算（误差约 0.81%，内存固定 ～12KB）

## 2、Redis 的事件机制

Redis 的事件机制是其高性能、单线程架构的核心支撑，它使得 Redis 能够高效处理网络 I/O、定时任务、文件事件等，而不会阻塞主线程。

### （1）为什么需要事件机制

Redis 是单线程（主线程）处理命令的，但它需要同时处理：
- 客户端连接（accept）
- 读取请求（read）
- 发送响应（write）
- 定时任务（如过期 key 清理、RDB 快照）
- 复制、集群心跳等

如果使用阻塞 I/O，一个慢客户端就会卡住整个服务。

解决方案：**基于 I/O 多路复用的事件驱动模型。**


### （2）Redis 事件的两大类型

#### （1）文件事件（File Event）

来源：**Socket 文件描述符（FD）上的 I/O 操作**

典型场景：
- 新客户端连接（accept）
- 客户端发送命令（read）
- 向客户端返回结果（write）
- 主从复制数据同步

底层依赖：I/O 多路复用（epoll / kqueue / select）

文件事件处理流程（I/O 多路复用 + 事件分发）：
- **注册事件**： 当新客户端连接时，Redis 将其 socket fd 注册到 aeEventLoop，并绑定处理器
~~~ c
aeCreateFileEvent(server.el, c->fd, AE_READABLE, readQueryFromClient, c);
~~~

- **事件循环**：主线程调用 aeMain() 进入无限循环：
~~~ c
void aeMain(aeEventLoop *eventLoop) {
    eventLoop->stop = 0;
    while (!eventLoop->stop) {
        // 处理文件事件（非阻塞）
        aeProcessEvents(eventLoop, AE_ALL_EVENTS);
    }
}
~~~

- **多路复用等待**：aeProcessEvents() 内部调用 aeApiPoll()（封装了 epoll_wait / kqueue）：
    - 等待 直到有 fd 就绪 或 超时（由最近的时间事件决定）
    - 返回就绪的 fd 列表

- **分发处理**：对每个就绪 fd，调用其注册的 handler：
    - readQueryFromClient() → 读取命令
    - sendReplyToClient() → 发送响应
    - acceptTcpHandler() → 接受新连接

文件事件处理总结：
**事件注册 -> 事件循环 -> 多路复用等待就绪文件描述符 -> 分发文件描述符进行相应处理**

eg：1000 个客户端同时发请求 → epoll_wait 一次性返回所有可读 fd → Redis 逐个处理，无阻塞。**即 socket 只有就绪了，IO多路复用才会返回其 fd 进行逐步处理，无阻塞，提升 Redis性能。**

#### （2）时间事件（Time Event）

来源：定时或周期性任

典型场景：
- 定期清理过期 key（每秒 10 次）
- RDB 自动保存（save 配置触发）
- 集群节点心跳（clusterCron）
- 客户端超时检测（timeout 配置）

#### （3）事件调度：如何兼顾文件事件和时间事件？

关键点：
**Redis 主线程在一个事件循环（Event Loop） 中，交替处理文件事件和时间事件。**

Redis 的事件循环采用 “尽力而为”策略：
- 计算最近时间事件的到期时间（如 50ms 后）
- 调用 aeApiPoll(timeout = 50ms)
    - 如果 50ms 内有 I/O 事件 → 立即处理
    - 如果 50ms 内无 I/O 事件 → 超时返回，处理时间事件
- 处理所有已到期的时间事件

**优先处理 I/O，时间事件可能延迟：时间事件不是严格准时的！如果文件事件处理耗时很长（如大 key 删除），时间事件会延迟执行。**

### （3）为什么单线程还能高性能？

**单 Redis 实例轻松支持 10w+ QPS，瓶颈通常在网络或内存，而非 CPU。**


高性能的三个主要原因：
- **单线程**：单线程管理数万连接，无上下文切换开销，无锁设计，CPU Cache 友好（L1/L2 缓存命中率高）
- **I/O 多路复用，且是非阻塞 I/O**：
  - I/O 多路复用（epoll）：单线程监听数万连接，只处理就绪的
  - 非阻塞 Socket：**读写不等待，避免线程挂起**
  
- **纯内存操作 + 高效数据结构**：数据在内存中，命令执行快（微秒级），无磁盘 I/O 等待

### （4）Redis 不完全是单线程

如下多线程情况：
- 持久化（BGSAVE/BGREWRITEAOF） → fork 子进程
- 异步删除（lazy free） → 后台线程
- I/O 线程（Redis 6.0+） → 可开启多线程处理网络 I/O（但命令执行仍是单线程）

核心思想：**命令执行逻辑单线程，避免数据竞争；I/O 和耗时操作可多线程卸载**

## 3、高可用-主从复制

Redis 的主从复制是其高可用、读写分离和数据冗余的核心机制。它允许一个或多个从节点实时同步主节点的数据，从而实现故障转移、负载分担和备份。

### （1）主从复制核心目标

- **数据冗余**：主节点宕机后，从节点可接管服务（需配合哨兵或 Cluster）
- **读写分离**：写操作走主节点，读操作可分发到从节点，提升吞吐量
- **灾备与备份**：从节点可作为冷备源，执行 BGSAVE 不影响主节点性能

### （2）基本架构

如下图：
~~~ md
         +-----------------+
         |   Master (主)    |
         +--------+--------+
                  |
     +------------+------------+
     |            |            |
+----v----+  +----v----+  +----v----+
| Replica1|  | Replica2|  | Replica3|  ← 从节点（只读）
+---------+  +---------+  +---------+
~~~

- 主节点可写可读，从节点默认只读
- 支持级联复制，即Replica 可再作为其他 Replica 的 Master

### （3）复制模式

#### （1）全量复制

触发场景：
- 从节点首次连接主节点
- 断开时间过长 → 主节点的 replication backlog 是环形缓冲区，旧数据被新数据覆盖 → 无法找回缺失命令，则使用 RDB 全量复制

过程：主节点生成 RDB 快照并传输给从节点

#### （2）部分复制（Partial Resynchronization）

触发场景：从节点短暂断线后重连，且主节点仍保留所需增量数据

依赖：复制偏移量（replication offset） + 复制积压缓冲区（replication backlog）**（环形缓冲区）**
优势：避免全量 RDB 传输，大幅提升网络中断恢复速度

说明：**从节点断开时间较短 → 主节点的 replication backlog 仍然包含从节点缺失的命令（即 [offset+1, current_offset] 在 backlog 范围内），主节点直接发送缺失的命令给从节点，无需 RDB 快照**


### （4）复制流程

#### （1）阶段 1：建立连接 & 身份校验

- 从节点配置 replicaof <master-ip> <master-port>
- 从节点向主节点发起 TCP 连接
- 主节点验证从节点身份（如 ACL、密码）

#### （2）阶段 2：判断全量 or 部分复制

从节点发送 PSYNC <runid> <offset>：
- runid：上次主节点的唯一 ID（首次为 ?）
- offset：从节点当前复制偏移量（首次为 -1）

主节点响应：
+FULLRESYNC <runid> <offset> → 全量复制
+CONTINUE → 部分复制

#### （3）全量复制流程

- 主节点执行 BGSAVE 生成 RDB 文件（子进程）
- 主节点将 RDB 文件通过 socket 发送给从节点
- 同时，主节点将新写命令缓存到 **复制客户端缓冲区**（replication buffer）
- RDB 传输完成后，主节点将缓冲区中的命令追加发送给从节点
从节点清空旧数据 → 加载 RDB → 执行增量命令 → 完成同步

#### （4）部分复制流程

- 主节点检查 replication backlog 是否包含 [offset+1, current_offset] 的命令
- 如果包含 → 直接发送缺失的命令给从节点
- 从节点执行这些命令，完成同步

### （5）关键数据结构与配置

- **复制偏移量**（Replication Offset）
主从各自维护一个计数器，记录已处理的命令字节数，用于判断数据是否一致、是否可部分复制

- **复制积压缓冲区**（Replication Backlog）
主节点专属的固定大小环形缓冲区（默认 1MB），存储最近的写命令，供断线重连的从节点“追赶”。配置：repl-backlog-size 1mb（建议根据写入量调大，如 100MB）
- 主节点 runid
每次重启会生成新 ID，从节点通过 runid 判断是否还是同一个主节点

### （6）主从复制的特点

- **异步复制**：主节点不等待从节点确认，可能丢数据（主宕机时）
- **无自动故障转移**：需配合 Sentinel（哨兵） 或 Cluster 实现高可用
- **从节点只读**：默认拒绝写命令
- **支持多从**：一个主可挂多个从，从可级联（树状拓扑）

## 4、高可用-哨兵&集群（cluster）

### （1）哨兵机制

#### （1）哨兵机制介绍
目标：实现 Redis 主从架构的**自动故障转移（Failover）**，保证高可用。

哨兵本身不存储数据，只负责监控、通知、自动故障恢复。架构如下：
~~~ md
+----------------+     +----------------+     +----------------+
|   Sentinel 1   |<--->|   Sentinel 2   |<--->|   Sentinel 3   |
+-------+--------+     +-------+--------+     +-------+--------+
        |                      |                      |
        v                      v                      v
+-------+--------+     +-------+--------+     +-------+--------+
|   Master       |     |   Replica 1    |     |   Replica 2    |
| (可写)         |     | (只读)         |     | (只读)         |
+----------------+     +----------------+     +----------------+
~~~

- 至少 3 个 Sentinel 节点（避免脑裂）
- 1 个主节点 + N 个从节点

哨兵的核心功能如下：
- 监控（Monitoring）：持续检查主从节点是否存活
- 通知（Notification）：节点故障时通知管理员或系统
- 自动故障转移（Automatic Failover）：主节点宕机 → 自动选一个从节点升级为主
- 配置提供者（Configuration Provider）：客户端通过 Sentinel 获取当前主节点地址

#### （2）故障转移的流程

- **主观下线（SDOWN）**：某个 Sentinel 发现主节点无响应（ping 超时），标记为 主观下线

- **客观下线（ODOWN）**：该 Sentinel 向其他 Sentinel 发起投票，超过半数 Sentinel 同意 → 标记为 客观下线

- **选举 Leader Sentinel**：所有 Sentinel 通过 Raft-like 协议选举一个 Leader，由 Leader 执行故障转移
选择新主节点

- **从节点中按优先级选择**：
    - 优先级高（slave-priority）
    - 复制偏移量大（数据最新）
    - runid 小（稳定性）

- **执行切换**
    - 新主节点执行 REPLICAOF NO ONE
    - 其他从节点执行 REPLICAOF new_master

- **更新 Sentinel 配置**：通知客户端，客户端下次查询主节点地址时，Sentinel 返回新主 IP

#### （3）优缺点

**哨兵的优点**
- 简单易用：只需部署 Sentinel 进程，无需改应用逻辑（配合客户端库）
- 自动故障恢复：主挂了，秒级切换
- 兼容现有主从：在原有主从基础上加 Sentinel 即可

**哨兵的缺点**
- 不支持数据分片（Sharding）：所有数据仍在单个主节点，内存和性能有上限
- 写能力无法扩展：只有一个主节点可写
- 运维复杂度：需额外维护 Sentinel 集群

适用场景：中小型系统，数据量 < 20GB，QPS < 10w，**要求高可用但无需水平扩展**。

### （2）集群机制

#### （1）集群机制介绍
核心目标：在保证高可用的同时，**实现数据分片（Sharding）和水平扩展。**

Cluster 既是存储层，又是协调层，内置分片与故障转移。

架构如下：
~~~ md
+-------------------+     +-------------------+     +-------------------+
|  Node 1 (Master)  |<--->|  Node 2 (Master)  |<--->|  Node 3 (Master)  |
|  Slots 0-5460     |     |  Slots 5461-10922 |     |  Slots 10923-16383|
+---------+---------+     +---------+---------+     +---------+---------+
          |                         |                         |
          v                         v                         v
+---------+---------+     +---------+---------+     +---------+---------+
| Node 4 (Replica)  |     | Node 5 (Replica)  |     | Node 6 (Replica)  |
+-------------------+     +-------------------+     +-------------------+
~~~

- 16384 个哈希槽（Slots），数据按 key 分配到槽
- 每个主节点负责一部分槽
- 每个主节点至少有一个从节点

#### （2）故障转移机制（Gossip + Raft）

- 节点间心跳（Gossip 协议）
  - 每秒向随机节点发送 PING
  - 传播节点状态（fail, master, replica 等）

- 主观下线（PFAIL）
  - 某节点发现主节点无响应 → 标记为 PFAIL

- 客观下线（FAIL）：通过 Gossip 传播，超过半数主节点同意 → 标记为 FAIL

- 从节点发起选举：从节点请求其他主节点投票（类似 Raft），获得多数票 → 升级为主节点

- 集群状态更新：新主广播 PONG，更新集群拓扑

#### （3）优缺点

**Cluster 的优点**
- 水平扩展：可轻松扩展到 1000+ 节点
- 高可用：每个分片有主从，自动故障转移
- 无中心节点：去中心化，避免单点瓶颈
- 写能力扩展：多个主节点可同时写入

**Cluster 的缺点**
- 客户端必须支持 Cluster 协议（处理 MOVED/ASK）
- 不支持多 key 操作跨 slot（如 MGET key1 key2 若在不同 slot 会报错）
- 运维复杂：扩容/缩容需重新分片（reshard）

适用场景：大型系统，数据量 > 50GB，QPS > 10w，**需要水平扩展和高可用。
**
### （3）哨兵与集群对比

![alt text](/复习资料/数据库/img/image-8.png)

## 5、缓存问题及其解决方案

### （1）缓存穿透

问题定义：大量请求查询根本不存在的数据（如 ID 为 -1 的用户），导致每次请求都绕过缓存，直接打到数据库。

成因
- 恶意攻击（如爬虫遍历无效 ID）
- 业务逻辑缺陷（前端传非法参数）
- 缓存未对“空结果”做处理

**故障场景**：黑客用脚本疯狂请求 user?id=999999999（不存在），数据库 QPS 瞬间飙升，CPU 打满，服务瘫痪。

解决方案：
- **缓存空值（Null Cache）**：对查询结果为空的 key，也缓存一个特殊值（如 "NULL"），设置较短 TTL（如 1～5 分钟，	简单有效，但是每个无效请求参数都要占用内存，可能被恶意利用填满缓存

- **布隆过滤器（Bloom Filter）**：在缓存前加一层布隆过滤器，快速判断 key 是否可能存在，内存高效，拦截 99% 无效请求，但是存在误判（假阳性），不能删除元素

- **接口层校验**：对 ID 做合法性校验（如范围、格式），从源头拦截	需业务配合，无法防所有无效 key

**面试话术：**
“我们通过布隆过滤器 + 空值缓存双重防护：布隆过滤器快速拦截明显无效请求，对漏网之鱼缓存空值防止反复查库。”

### （2）缓存击穿

问题定义：某个热点 key 在缓存过期瞬间，大量并发请求同时发现缓存失效，全部打到数据库。

**注意：和“穿透”不同，key 是存在的，只是刚好过期。**

**成因：**
- 热点数据（如秒杀商品、首页 banner）设置了固定 TTL
- 高并发下缓存过期时间集中

故障场景：双十一 0 点，10w 用户同时刷新“iPhone 抢购页”，缓存刚好在 00:00:00 过期 → 10w 请求压垮 DB。

解决方案：
- 互斥锁（Mutex Lock）：第一个线程发现缓存失效，加锁去查 DB 并回填缓存，其他线程等待或重试，强一致性要求高

- 热点 key 永不过期：对已知热点数据，手动设置 long TTL 或永不过期，适用于可预知热点

面试话术：“对热点 key，我们采用互斥锁重建缓存，确保同一时间只有一个线程查库，其余线程等待缓存就绪。对于已知的热点 key，设置永不过期。”

### （3）缓存雪崩

问题定义：大量缓存 key 在同一时间失效，或整个缓存集群宕机，导致所有请求瞬间压向数据库。

和“击穿”区别：**击穿是单个 key，雪崩是大批量 key 或整个缓存失效。**

**成因：**
- 所有 key 设置了相同的 TTL（如 2 小时），如批量导入数据后统一设置缓存
- Redis 集群宕机（如主从全挂）

故障场景：系统重启后，100w 商品缓存同时设置 TTL=3600s → 1 小时后集体失效 → DB 被打爆。

**解决方案：**

- **TTL 加随机值**：基础 TTL + 随机偏移（如 3600 ± 300 秒），分散过期时间
- **多级缓存**：本地缓存（Caffeine） + Redis，即使 Redis 挂了，本地缓存可扛几秒
- **高可用架构**：Redis 哨兵 / Cluster 避免单点故障
- **熔断降级**：数据库压力过大时，返回兜底数据或错误页（如 Hystrix）

面试话术：我们通过TTL 随机化 + 多级缓存 + Redis 高可用三重保障，避免缓存集体失效或集群宕机引发雪崩。

### （4）缓存污染 / 缓存满了

问题定义：缓存中存储了大量低频访问或无用数据，挤占了热点数据的空间，导致缓存命中率下降。

**成因：**
- 一次性扫描全表数据（如报表导出）写入缓存
- 恶意用户遍历大量冷数据，导致冷数据进入缓存挤占缓存空间

故障场景：运营导出 100w 用户数据，每个用户都写入缓存 → 热点商品缓存被逐出 → 商品页变慢。

**解决方案：**
- **分层缓存池**：热点数据用独立缓存实例（如 Redis DB 0），冷数据用另一实例（DB 1）
- **淘汰策略优化**：使用 allkeys-lru（全局 LRU）而非 volatile-lru（仅带 TTL 的 key）
- **手动标记冷热**：业务层标记数据热度，冷数据不进缓存或设短 TTL，如只缓存访问 ≥2 次的数据

### （5）缓存与数据库一致性

问题定义：数据库和缓存更新，就容易出现缓存(Redis)和数据库（MySQL）间的数据一致性问题。不管是先写MySQL数据库，再删除Redis缓存；还是先删除缓存，再写库，都有可能出现数据不一致的情况。

- 如果删除了缓存Redis，还没有来得及写库MySQL，另一个线程就来读取，发现缓存为空，则去数据库中读取数据写入缓存，此时缓存中为脏数据。
  
- 如果先写了库再删缓存，如果写库速度慢，会有短暂的事件读取到旧数据，但是更新成功删除缓存后，新来的线程就会从数据库读取到新的数据，不会产生永久脏数据，这种情况通常是可接受的。

**更新缓存的四种 Design Pattern**
如下：
- Cache aside：旁路缓存
- Read through：读穿透
- Write through：写穿透
- Write behind caching：写后缓存

#### （1）Cache aside（旁路缓存）

核心思想：**应用层直接管理缓存与数据库的读写逻辑，缓存作为“旁路”存在，不参与核心数据流。**

**读流程（Read）：**
- 先读缓存
- 若命中 → 返回
- 若未命中 → 读数据库 → 回填缓存 → 返回

**写流程（Write）：**
- 先更新数据库
- 再删除缓存（**不是更新！**），不更新缓存，是因为更新缓存成本高（需查 DB 拼完整对象），删除后下次读自动回填，更简单可靠

**优点：**
- 简单、灵活、可控
- 避免写缓存失败导致数据丢失（先删缓存，更新数据库失败查不到数据）
- 不会出现永久性脏数据（更新数据库成功后，会将脏数据删除重新加载）

**缺点**：存在短暂不一致窗口，且需要应用层编码处理

#### （2）Read Through（读穿透）

核心思想：缓存层代理读请求。若缓存未命中，由缓存组件自动从数据库加载并返回。

**读流程**：应用只读缓存，缓存未命中 → 缓存组件调用预设的 load() 方法查 DB，自动回填并返回

**优点：** 应用层无感知，代码简洁，适合通用缓存框架（如 Ehcache、Guava Cache）

**缺点：** 缓存组件需集成 DB 访问逻辑（耦合），不适合复杂业务查询（复杂业务直接操作缓存比较合适），缓存宕机则读失败

#### （3） Write Through（写穿透）

**核心思想**：写操作先写缓存，由缓存组件同步写入数据库。

**流程：** 应用写缓存，缓存组件调用 write() 方法，同步写 DB，成功后才返回应用

**优点：** 数据强一致（写成功即 DB 和缓存都更新），应用层简单

❌ 缺点：
- 写性能低（每次写都要等 DB）
- 缓存组件需实现 DB 写逻辑（耦合）
- 缓存宕机 → 写失败

读穿透与写穿透配合使用，适用场景： **对一致性要求极高、写少读多的场景（如配置中心）**

#### （4）Write Behind Caching（写后缓存）

核心思想：**写操作只更新缓存，异步批量刷入数据库。**

流程：
- 应用写缓存（立即返回）
- 缓存组件在后台异步、批量写 DB

**优点：** 写性能极高（无 DB I/O 延迟），合并写操作，减少 DB 压力

缺点：**可能丢数据（缓存宕机未刷盘）**

适用场景：日志、监控、计数器等允许丢失的场景

#### （5）Cache aside（旁路缓存）+延迟双删

旁路缓存可能出现的2种问题：

- DB 单节点，可能出现短暂不一致

![alt text](/复习资料/数据库/img/image-9.png)


- DB 主从，缓存里面可能一直是脏数据

![alt text](/复习资料/数据库/img/image-10.png)

**延迟双删方案流程：**

- 第一次删除缓存
- 更新数据库
- 延迟 N 毫秒（如 500ms），这里等待数据库主从同步完成
- 第二次删除缓存

作用：
第一次删：清除当前可能存在的旧缓存
延迟后删：**清除因主从延迟或并发读在更新 DB 后、第一次删之前回填的旧缓存**

**延迟时间 > 主从同步最大延迟 + 读请求处理时间，通常设为 300～1000ms**

















