## 1、设计一个秒杀系统

### 1、核心设计原则

设计秒杀系统，要遵从3个原则
- **高可用**：秒杀业务挂了，不能影响主站业务（比如普通的搜索、下单）。结论：服务隔离，数据库隔离。
- **高性能**：请求必须在到达数据库前被拦截，全链路低延迟。
- **强一致性**：绝对不能超卖（卖出 101 个商品），尽量也不能少卖。

### 2、架构设计（漏斗模型）

我们要像漏斗一样，把巨大的流量一层层拦在外面。

#### （1）客户端（App/H5）—— 视觉欺骗与拦截

- **按钮控制**：点击“立即抢购”后，按钮立刻置灰 5 秒。作用：拦住 90% 的手抖用户和重复点击。

- **静态资源 CDN**：秒杀页面的 CSS/JS/图片 全部推送到 CDN，不打后端服务器。

- **动态 URL**：不要写死 /seckill/sku_123，防止脚本提前刷接口。 URL 应该是动态获取的（见下文“防刷”）。

#### （2）接入层（Nginx/Gateway）—— 流量清洗
- **IP 限流**：限制单 IP 每秒请求数。

- **黑名单**：识别爬虫 User-Agent 或异常 IP，直接返回 403。

- **动静分离**：只放行 API 请求给后端，静态请求全部由 Nginx 挡回去。

#### （3）服务层

- **请求队列**：如果瞬间流量太大，可以引入内部队列或线程池排队，抛弃多余请求。

- **缓存层（Redis）—— 核心战场**：利用 Redis DECR 原子递减，或者 Lua 脚本。只有在 Redis 扣减成功的（stock >= 0），才有资格进入下一步。

- **消息队列（MQ）—— 削峰填谷**：通过 MQ 异步下单，抢到 Redis 库存的用户，不是立刻写数据库，而是发一条消息到 MQ（RabbitMQ/Kafka）。立即返回：告诉用户“排队中/抢购中”，前端启动轮询器（Polling）查询最终结果。

#### （4）数据库层

Consumer 消费 MQ，执行最终的 SQL 并生成订单，在 DB 插入订单记录。插入业务要进行幂等性验证，同一个用户不可以重复下单。

### 3、关键痛点解决方案

#### （1）如何解决“超卖”问题

在 Redis 缓存层，通过 lua 脚本将“判断库存”和“扣减库存”做成原子操作。

~~~ lua
if (redis.call('get', KEYS[1]) > 0) then
    redis.call('decr', KEYS[1]);
    return 1;
else
    return 0;
end
~~~

数据库兜底： 即使 Redis 扣成功了，DB 写入时也要加保险，创建订单时要判断库存大于0
~~~ java
UPDATE stock_table SET count = count - 1 WHERE id = 123 AND count > 0;
~~~

#### （2）如何防止“少卖”？ (库存长时间被占用但不支付)

场景：Redis 扣了库存，用户占了坑，结果 30 分钟不支付。

解法：库存回滚。用户订单超时未支付，执行订单到期关闭。订单超时关闭后，必须发消息给库存服务，把 Redis 的库存 INCR 加回去，并把 DB 库存加回去。

#### （3）如何防止脚本/机器人抢购

**数学验证码**：在抢购前弹出一个简单的数学题（12+8=?），防止简单脚本高频重放。

**隐藏秒杀地址，不要写死秒杀接口地址**：
- 秒杀开始前，按钮不可点。
- 点击按钮，先请求 /get_token 接口，服务器根据 UserID + 商品ID + 盐 算出一个随机 Token（MD5）。
- 真正的下单接口是 /seckill/verify_token/{token}。没有这个 Token，直接拒绝。

#### （4）Redis 挂了怎么办

- **主从/集群架构确保 Redis 高可用**：基本的 Redis 高可用。

- **本地缓存兜底**：如果 Redis 真的全挂了，Switch 降级，由应用层本地缓存直接拦截所有请求，虽然秒杀失败，但系统没挂（弃车保帅）。

### 4、面试问题

#### （1）面试官问“你讲讲秒杀系统的核心思路？”

回答话术： “秒杀系统的核心在于**流量削峰和读写分离**。

**架构上**：我采用漏斗模型。前端拦截 90% 请求，网关限流拦截 5% 请求，Redis 预扣库存处理核心流量，最后只有极少数请求（等于库存数）才会打到数据库。

**数据上**：利用 Redis 原子性（Lua 脚本）做库存预扣，利用 MQ 做异步下单，将突发的写流量转化为平滑的流式写入，保护后端数据库不被击穿。”

#### （2）面试官追问“如果 Redis 扣减成功，但 MQ 写数据库失败了怎么办（数据不一致）？”

回答话术： “这涉及分布式一致性。

- 本地消息表：在发送 MQ 前，先在本地库插一条‘消息发送记录’（事务内）。
- **MQ 可靠投递**：利用 MQ 的 confirm 机制保证消息发出。
- **消费端重试**：如果 DB 写失败，让 MQ 重试。
- **最终一致性**：如果实在写不进去（比如 DB 挂了），要有脚本进行对账。活动结束后，比对 Redis 扣减数量和 DB 订单数量，人工介入处理。 在秒杀场景下，我们优先保证不超卖，对于极个别的数据不一致，可以通过人工补偿解决。”

#### （3）面试官问“怎么解决一个用户用多个账号同时抢？”

回答话术： “这属于风控范畴。

- **限制设备/IP**：同一个设备指纹或同一个 IP，限制只能登陆 1 个账号。

- **业务限流**：在 Redis 里记录 userId_skuId，设置 nx（setIfAbsent），限制单用户对单商品只能请求一次。

- **风控服务**：接入阿里的风控或自研风控，基于历史行为识别僵尸号，直接在网关层拦截。”


## 2、库存扣减如何避免超卖和少卖

### 1、考虑的问题维度

面试官问这个问题，其实是在问三个维度的东西：

- **并发控制**：怎么防止多线程把库存扣成负数？（防超卖）

- **业务流程**：什么时候扣库存？下单扣还是支付扣？（防少卖/用户体验）

- **异常处理**：订单取消了，库存怎么还回去？（库存回滚，防少卖）

### 2、如何避免超卖

#### （1）方案一：数据库乐观锁 (Optimistic Locking) —— 中小规模推荐

利用 SQL 语句本身的原子性，数据库会保证这一行更新时是串行的。
~~~ java
UPDATE stock_table
SET count = count - num
WHERE sku_id = 123
  AND count >= num;  -- 核心：带上版本号或数量判断
~~~

#### （2）方案二：Redis + Lua 脚本 —— 大规模高并发推荐

当并发量达到上万 QPS，直接打数据库会挂。必须在 Redis 层拦截。Redis 执行 Lua 脚本是单线程原子的，中间不会被插入其他命令。


### 3、如何防止“少卖”

防止少卖的核心在于库存的释放机制。这涉及到“什么时候扣库存”。

#### （1）库存扣减时机选择

这里有三种流派，面试一定要分析优缺点：

- **下单扣库存 (Recommended)**

**优点**：用户体验最好，下单成功就一定能买到，不会出现支付时告诉没货。

**缺点**：可能遭遇恶意刷单。用户下完单不支付，把库存占着。

**对策**：必须配合订单有效期（如 15 分钟）。超时未支付，自动取消订单，释放库存。

- **支付扣库存**

**优点**：不会被刷单，卖出去的一定是真金白银。

**缺点**：用户体验极差。下单成功了，去支付的时候告诉你没货了（因为被别人抢先付了）。**电商通常不采用。**

- **预扣库存 (Pre-deduct)**

下单时扣 Redis 库存，支付成功后扣 DB 库存。大致的流程如下：
~~~ text
用户下单
    ↓
[1] 检查 Redis 库存是否充足（GET stock）
    ↓ 否 → 返回“库存不足”
    ↓ 是
[2] Redis 预扣库存（DECR stock）→ 同时写入“预占记录”（如 Hash: order_id → sku_id, expire_time）
    ↓
[3] 创建订单（DB 状态 = "待支付"）
    ↓
[4] 用户跳转支付
    │
    ├─ 支付成功 → [5] MQ 异步回调：扣减 DB 库存 + 清理 Redis 预占记录
    │
    └─ 支付失败 / 超时未支付 → [6] 定时任务：释放 Redis 预占库存
~~~

#### （2）库存回滚

为了防止少卖，必须有可靠的回滚机制。用户下单后（Redis 库存 -1），15 分钟未支付，则需要进行库存的回滚。有以下两种方式：

##### （1）库存回滚方式

- 定时任务扫描（推荐）

每隔固定时间（如 1 分钟）扫描 DB 中 “待支付”且超时 的订单，超时时间 = 下单时间 + 支付有效期（通常 15~30 分钟）

- 延迟队列（更实时）

下单时向 MQ（如 RabbitMQ / RocketMQ / Redis ZSet）发送一条 延迟消息（30 分钟后触发），时间到 → 自动触发关单。

优点：无轮询开销，响应更快

生产建议：两者结合 —— **延迟队列为主，定时任务为兜底**。

##### （2）库存回滚流程

**步骤 1：检查是否已支付（幂等性）**

查询 DB 订单状态，若已是 PAID 或 CANCELLED → 直接跳过，避免重复处理（如支付回调和关单并发）

**步骤 2：释放 Redis 预占库存（原子操作）**

使用 Lua 脚本 确保 “检查预占记录 + 回滚库存 + 删除记录” 原子执行。

**步骤 3：更新订单状态为“已关闭”**

### 4、总结

如果让我设计一个秒杀级的库存系统，我会采用 **Redis 预扣 + MQ 异步同步 + 数据库兜底** 的方案：

**1、初始化**：活动开始前，将数据库库存 warm-up（预热）到 Redis。

**2、下单扣减 Redis 库存：**

前端发起下单请求，后端执行 Redis Lua 脚本扣减库存。

- 如果库存不足：直接报错“库存不足”。

- 如果库存充足：先在 Redis 中插入一条预扣库存记录，同时Redis扣减库存，并发送一条“创建订单”的消息到 MQ，并立即给前端返回“排队中”。同时发送一条关单的延迟消息到 MQ。

**3、支付真实扣减 DB 库存：**

**支付成功**：则向 MQ 发送一条消息，异步扣减库存，并修改订单为支付成功状态，更新DB库存时候使用乐观锁进行兜底；

**超时未支付or支付失败**：关单的延迟 MQ 消息到达，检查订单是否支付，如果未支付，回滚Redis库存，同时关闭订单。同时要有定时任务去扫描超时未支付的订单，进行兜底。

**4、兜底策略：**

要有对账脚本定时进行 Redis 和 DB 库存和订单的对账，必要时候引入人工对账确保业务的正确性。 

### 5、面试问题

#### （1）面试官问“下单扣库存还是支付扣库存？怎么防少卖？

为了保证用户体验，我们通常采用下单扣库存（Lock Stock）。但这会带来‘恶意占用’导致少卖的问题。

我们采用的先扣减 Redis 库存，支付成功再异步扣减 DB 库存的方案。
- 当支付成功的时候，异步扣减 DB 库存，避免大流量直接冲击DB

- 支付失败或者超时未支付，使用MQ延迟消息或者定时任务的方式进行关单，使用 LUA 脚本恢复Redis库存，这样可以有效控制库存避免少卖。

#### （2）面试官问“如果 Redis 扣减成功，但数据库写失败了，库存怎么一致？”

原因：DB 故障、网络超时、死锁等

后果：Redis 库存已扣，但 DB 未扣 

解决方案：
- **MQ 重试机制**：如果 MQ 消息消费失败，即写库失败（如网络延迟），这里有 MQ 的重试机制，可进行消费重试。如果一直消费失败（数据库挂了），消费者重试多次后进入死信队列。

- **对账脚本or人工对账**：我们需要一个定时任务（对账脚本），在活动结束后比对 Redis 的扣减量和数据库的实际订单量。如果发现数据库少单了，进行人工介入或自动回滚 Redis 库存。

**最终：以 DB 为准，若 DB 扣减失败，需回补 Redis 库存**

## 3、40亿个QQ号，限制1G内存，如何去重

### 1、QQ 号存储类型深度分析

业界推荐在数据库（MySQL）中使用 String（或 VARCHAR）存储 QQ 号，但在内存计算算法中，选择会有所不同（**当前的业务是要在内存中对 40 亿个 QQ 号进行去重，不是存储到数据库**）。

#### （1）int (Integer)

存储：4 Bytes (32 bits)。

范围：
- 有符号 (Signed)：$-21亿 \sim 21亿$。
- 无符号 (Unsigned, Java 8+ 支持)：$0 \sim 42.9亿$ ($2^{32}-1$)。
 
QQ现状：早期的 QQ 是 5位、6位，现在普遍是 9位、10位。最大值已经突破了 21 亿（例如 3开头、4开头的 10 位 QQ）。

缺点：存不下。Java 的 int 默认是有符号的，超过 21 亿就会溢出变成负数，导致逻辑错误。

结论：不能直接用 Java 的 int，除非你确定数据都在 21 亿以内。

#### （2）long (Long)

存储：8 Bytes (64 bits)。

范围：$\pm 922亿亿$。涵盖 10 位甚至 20 位 QQ 绰绰有余。

优点：数值计算快，无溢出风险，完全覆盖所有 QQ。

缺点：内存占用是 int 的 2 倍。

结论：算法计算时的首选数值类型。

#### （3）String (字符串)

存储：
- 数据库：推荐。因为不仅能存数字，还能兼容未来可能出现的“字母+数字”规则，或者保留前导零（虽然 QQ 没有）。

Java 内存对象：巨坑。

- 一个 String 对象包含：对象头(12B) + Hash值(4B) + 长度(4B) + 字符数组引用(4B) + 字符数组本身。存一个 "1234567890" 的字符串，Java 堆内存至少消耗 48~64 Bytes。

缺点：极其浪费内存。

结论：业务存储用 String，但在这个面试题（内存受限）中，是要在内存中对 40 亿 QQ 号进行去重，绝不能用 String。

### 2、数值分析

面试第一步，先用数据证明你对“内存”和“数据规模”有绝对的掌控力。

#### （1）数据特征分析

数量：40 亿个（Count = $4 \times 10^9$）。

范围：QQ 号目前是 5~10 位数字，最大值理论上接近 100 亿 ($10^{10}$)。

类型陷阱：Java 的 int 最大值是 21 亿 ($2^{31}-1$)，存不下 10 位 QQ 号。必须使用 long 类型。

#### （2）内存瓶颈计算（核心矛盾点）

1GB 内存能存多少个 Bit：

$$1 \text{ GB} = 1024^3 \text{ Bytes} \times 8 \text{ bits/Byte} \approx \mathbf{85.8 亿 \text{ bits}}$$

我们需要多少个 Bit：如果要用 BitMap 覆盖所有可能的 QQ 号（0 ~ 99.99亿），我们需要约 100 亿个 bits。

结论：
$$100 亿 \text{ (需求)} > 85.8 亿 \text{ (上限)}$$

直接申请一个大 BitMap 会导致内存溢出 (OOM)。

方案推导：如果使用 bitMap 进行去重，不能一次性装入，必须采用 “时间换空间” 的策略，使用分治法，将数据分为 2 段（或多段）加载。

### 3、核心方案 —— 分段位图

这是最标准、最高效的解法，专门解决“内存不够装下全量 BitMap”的问题。

#### （1）核心思路

既然 1G 内存只能装下 85 亿个数字的状态，而 QQ 号最大有 100 亿。我们将 0~100亿 的范围切分为两段：

第一段：0 ~ 50亿

第二段：50亿 ~ 100亿

#### （2）详细执行步骤

Pass 1（第一轮遍历）：
- 申请内存：申请一个大小为 600MB 的 BitMap（对应 $50亿 / 8$ 字节）。这远小于 1GB，安全。
  
- 遍历数据：读取 40亿 个 QQ 号。
  - 逻辑：如果 QQ 号 $num < 50亿$：
  - 查看 BitMap.get(num)。如果为 1，说明重复；
  - 如果为 0，执行 BitMap.set(num, 1)。
  - 如果 QQ 号 $\ge 50亿$：跳过不处理。
- 清理：第一轮结束后，清空 BitMap 内存。

Pass 2（第二轮遍历）：

- 申请内存：再次申请 600MB BitMap。
  
- 遍历数据：再次读取 40亿 个 QQ 号（需要读第二次文件）。
  - 逻辑：如果 QQ 号 $num \ge 50亿$，计算偏移量 offset = num - 50亿。
  - 查看 BitMap.get(offset)。如果为 1，说明重复；
  - 如果为 0，执行 BitMap.set(offset, 1)。
  - 如果 QQ 号 $< 50亿$：跳过。

#### （3）优缺点

优点：内存绝对安全（峰值仅 600MB），算法极其简单高效。

缺点：IO 翻倍（需要遍历两遍大文件）。但在内存受限的极端场景下，这是唯一解。

### 4、进阶方案 —— Roaring BitMap (压缩位图)

Roaring Bitmap 是一种高性能、高压缩率的位图（Bitmap）数据结构，专为解决传统位图在稀疏数据场景下内存浪费严重的问题而设计。

它被广泛应用于 OLAP 数据库（如 ClickHouse、Druid）、搜索引擎、用户画像系统 等需要高效处理海量整数集合的场景。

#### （1）传统位图的痛点

假设要存储两个整数：{1, 1_000_000}，传统位图需分配 1_000_001 / 8 ≈ 125 KB 内存，但实际只用了 2 个 bit → 内存利用率 < 0.002%。

💡 问题本质：传统位图对“稀疏”和“密集”数据一视同仁，无法自适应压缩。

#### （2）Roaring Bitmap 核心思想

**分而治之 + 混合存储**

将 32 位无符号整数（0 ~ 2³²−1）的整个空间划分为 2¹⁶ = 65536 个桶（buckets），每个桶负责一个 16 位高段（high 16 bits） 的范围：
- 整数 x → 高 16 位 = x >>> 16（桶 ID）
- 低 16 位 = x & 0xFFFF（桶内偏移）

例如：
x = 100,000 → 高 16 位 = 1，低 16 位 = 34,464
x = 1,000,000 → 高 16 位 = 15，低 16 位 = 16,928

每个桶内部根据元素数量动态选择最合适的存储方式：

| 桶内元素数量 | 存储结构 | 说明 |
|-------------|--------|------|
| ≤ 4096 个 | Array Container（短数组） | 存储有序的低 16 位值 |
| > 4096 个 | Bitmap Container（位图） | 65536 位 = 8 KB 位图 |

阈值 4096 的由来：
- short Array：每个元素占 2 字节 → 4096 × 2 = 8192 字节
- Bitmap：固定 8192 字节（65536 / 8）

→ 两者内存相等，以此为界最优！

#### （3）两种 Container 详解

##### 1. Array Container（短数组）

适用：桶内元素 ≤ 4096

结构：short[] 存储有序的低 16 位值

优点：
- 内存紧凑（n 个元素占 2n 字节）
- 插入/查询用二分查找（O(log n)）

~~~ java
// 桶高段 = 1，包含 {1000, 5000, 10000}
short[] values = {1000, 5000, 10000}; // 已排序
~~~

##### 2. Bitmap Container（位图）

适用：桶内元素 > 4096

结构：long[1024]（1024 × 64(long 占 8 个字节 64位) = 65536 位）
优点：
- 查询/插入 O(1)
- 位运算（AND/OR/XOR）极快

内存固定：8192 字节


#### （4）总结（一句话）

Roaring Bitmap 通过将整数空间分桶，并为每个桶智能选择“数组”或“位图”存储，实现了稀疏数据的极致压缩与密集数据的高效运算，是现代大数据系统中处理整数集合的首选数据结构。

### 5、通用方案 —— Hash 分治法

如果面试官说：“不仅仅是 QQ 号，可能是 40亿 个 URL 字符串，或者无法预知最大值的数字。” —— 这时候 BitMap 失效，必须用通用解法。

#### （1）核心思想：Divide and Conquer（分治）
大文件进不去内存，就把它切成 100 个小文件，保证每个小文件能装进内存。

#### （2）步骤

- 文件拆分 (Splitting)：
    - 遍历 40亿 数据，计算 Hash(Data) % 100。将数据追加写入到 file_0, file_1 ... file_99 这 100 个小文件中。
    - 关键点：相同的 QQ 号，Hash 值一定相同，所以一定会被分到同一个小文件里。

- 内存去重 (Processing)：

依次将每个小文件加载到内存。因为每个文件平均只有 4000万 数据，使用 Java 的 HashSet（存 Long 或 String）即可，不需要 BitMap。

处理完一个文件，清空内存，处理下一个。

#### （3）优缺点

优点：通用性强，支持 String、IPv6 等非数字类型。

缺点：极慢。涉及大量磁盘 IO（读大文件 -> 写小文件 -> 读小文件）。

### 6、面试官攻防

#### （1）你刚才提到了 Int 和 Long，为什么 QQ 号不能用 Int？

回答话术：“这是一个由于数据增长带来的历史遗留问题。Java 中 int 是有符号 32 位整数，最大值是 $2^{31}-1$（约 21 亿）。而现在的 QQ 号已经发放到 10 位数，开头是 3 或 4，数值远超 21 亿。如果强行用 int 会溢出变成负数，导致下标越界或逻辑错误。所以在 Java 中处理 QQ 号，必须使用 long 类型，或者在 BitMap 索引计算时强转为 long。”

#### （2）为什么不直接用 String 存 QQ 号？

回答话术： “在 1G 内存限制下，用 String 是自杀行为。 String 对象头 + 字符数组引用 + 内部 char[] 数组，存一个 10 位 QQ 号至少消耗 50 字节。 40 亿个 String 需要 200 GB 内存。 而用 BitMap 的话，一个 QQ 号只占 1 bit。两者的空间效率相差 400 倍以上。”

#### （3）如果我一定要在 1G 内存内一次性跑完，不允许遍历两遍文件，怎么办？

回答话术： “如果这是硬性指标，我会尝试使用 Roaring BitMap。 因为 40 亿数据相对于 100 亿的地址空间来说，并不是完全稠密的（密度约 40%）。Roaring BitMap 对稀疏区域使用数组存储，很有可能将总内存压缩到 1GB 以内。 但最保险的工程方案，依然是分段 BitMap，因为它对内存的消耗是确定性的（稳定 600MB），不会因为数据分布的随机性导致突然 OOM。”

#### （4）如果是 IPv6 地址去重呢？

回答话术： IPv6 是 128 位整数（IPv4 是一个 32 位无符号整数），BitMap 存不下（需要天文数字的内存）。 必须使用 Hash 分治法（切分为几千个小文件），然后分别加载小文件到内存，可以使用 HashSet 进行处理。

或者如果是允许极低误判率的场景（如爬虫去重），可以使用 **布隆过滤器 (Bloom Filter)**，它能在极小的内存下判断**可能存在 或 一定不存在**。





























































